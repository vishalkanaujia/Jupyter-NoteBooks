{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bluesotre Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If WAL is full what would happen? Would writes block?\n",
    "\n",
    "It never blocks; it will always just spill over onto the next fastest \n",
    "device (wal -> db -> main). Note that there is no value to a db partition \n",
    "if it is on the same device as the main partition.\n",
    "\n",
    "> Would a drastic (quick) action to correct a too-small-DB-partition\n",
    "> (impacting performance) is to destroy the OSD and rebuild it with a\n",
    "> larger DB partition?\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would check your running Ceph clusters and calculate the amount of objects per OSD.\n",
    "total objects / num osd * 3\n",
    "\n",
    "For the moment though, having multiple (4) \n",
    "256MB WAL buffers appears to give us the best performance despite \n",
    "resulting in large memtables, so 1-2GB for the WAL is right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A tool to gather complete Ceph cluster information\n",
    "https://github.com/42on/ceph-collect\n",
    "    \n",
    "Bluesotre onode size is 24k for average object size of 2.8MB in RBD. So average object size and count per TB can be calculated.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## http://ceph-users.ceph.narkive.com/8uPMEXNz/bluestore-osd-data-wal-db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
